# [agent.exe](https://github.com/corbt/agent.exe)

以下では、まず「**非エンジニアの方**向け」に、このプロジェクト（Agent.exe）がどんなことをするものか、ざっくりした説明を行い、その後「**エンジニアの方**向け」に、少し踏み込んだロジックの仕組みを解説します。

---

## 非エンジニア向け：Agent.exeで何ができるの？

- **AIが自分のパソコンを操作してくれるツール**です  
  - 具体的には、ClaudeというAIモデル（Anthropic提供）に「パソコンで○○してほしい」と命令すると、AIが実際にマウスやキーボード操作を行ってくれます。  
  - 例：「ブラウザで◯◯のサイトを開いて検索して」や「ファイルを開いて編集して」などを実際に“自動的に”マウス移動＆キーボード入力でやってくれます。

- **画面キャプチャをAIに送り、状況を把握させる**  
  - AIは一歩ごとにスクリーンショットを見ながら「正しくできたかな？」と考え、必要に応じて次のマウス移動やキー入力などを繰り返します。  
  - なので、「ここをクリック」「文字をタイプ」など自動操作が連続して進みます。

- **完全自動で実行できるが、ストップも可能**  
  - 一応設定上、「よしなに全部やっちゃって」とAIに任せるフルオートモードが想定されていますが、実装は「とりあえず全自動」で動きます。もしうまく動いていないと思ったら`Stop`ボタンで停止できます。

- **セットアップも比較的シンプル**  
  1. レポジトリをダウンロードし（`git clone ...`）、  
  2. 依存ライブラリをインストール（`npm install`）、  
  3. `.env`ファイルにAI APIキーを設定、  
  4. そして`npm start`を実行するだけでElectronのアプリが起動します。

> **注意点**  
> - AIにパソコン全体を操作させるため、**危険**な操作をする可能性もゼロではありません。あくまで“実験的”なツールです。  
> - ClaudeがFirefoxブラウザを好む傾向があるらしく、Firefoxを入れておくとスムーズに動く場合があります。  
> - 画面の解像度や環境によっては思わぬ挙動になることもあり得ますのでご注意ください。

---

## エンジニア向け：ロジックの仕組み

### 1. 大きな流れ
1. **ユーザーが指示（指示文）を入力**  
   - 例：「○○のウェブサイトに行ってログインして、▲▲を検索」など
2. **Anthropic API（Claude 3.5モデル）とやりとり**  
   - いわゆるチャット形式ですが、“普通のテキスト応答” ではなく「ツール呼び出し」としてのメッセージが返ってきます。
3. **ツール呼び出しの内容に応じて、Nut.js がマウス＆キーボード操作を実行**  
   - Claudeから「マウスを左クリック」「ここにテキストをタイプ」「Enterキーを押す」などの具体的な指示が返され、それを実際に行う。
4. **操作実行後、スクリーンショットを撮ってClaudeへ送り返す**  
   - Claudeが「自分がクリックした結果画面はどうなったか」を見て、次ステップを判断する――という流れをループします。
5. **完了したら`finish_run`ツール呼び出し**で終了  
   - Claude側が「目的を達成したよ」と判断すると終了のAPI呼び出しを行い、アプリ側も操作ループを切ります。

### 2. 具体的なファイル解説

- **`src/main/runAgent.ts`**
  - ここが「AIと操作のやりとり」をループする中心的なファイルです。  
  - `promptForAction()`関数で「次のステップの実行指示」をClaudeに問い合わせ→返ってきたメッセージを `extractAction()`関数で解析 → Nut.jsなどで操作実行 → スクショを再度AIに送る … を繰り返す仕組みになっています。

- **`extractAction.ts`**
  - Claudeが`tool_use`形式で「computer」ツールを呼び出した際のパラメータを解析し、「マウス移動」「クリック」「キーボード入力」「終了」などを示す **`NextAction`** 型に変換します。
  - もし他のツールや不正なツールを呼ばれたらエラーにしたり、終了用のツール(`finish_run`)が呼ばれたら終了と判断している。

- **Nut.js（`@nut-tree-fork/nut-js`）**  
  - `performAction()`内部でNut.jsのAPIを呼び出し、マウス移動/ドラッグ/キーボード打鍵などを行っています。  
  - たとえば `mouse.setPosition()` で座標指定移動や `keyboard.type()`で文字打鍵が可能。

- **スクリーンショットの取得（`desktopCapturer` + 画像縮小）**  
  - `desktopCapturer`を使ってデスクトップ全体をキャプチャし、メインモニターの解像度を `mapToAiSpace()`/`mapFromAiSpace()` 関数でスケーリング調整し、Claudeに送る際は**Base64エンコードしたPNG**データを渡します。

- **Zustand + Zutron**  
  - メインプロセスの状態（AI実行中かどうか、エラーは何か、指示履歴など）を`Zustand`で管理し、`zutron`ライブラリによってレンダラープロセスと同期します。  
  - レンダラーではReact Hookを介して`useStore()`として読み書き可能になり、`dispatch()`関数でメインプロセスのアクションを呼び出す仕組み。

### 3. 留意点

- **安全性**  
  - AIがマウス/キーボード操作を「いったいどんな順序で行うか」を人間が制御しきれない可能性があります。  
  - 完全に任せると、ファイル削除や不要な操作をするリスクがあるため、十分注意が必要です。
- **座標スケーリング**  
  - Claudeには「ディスプレイ解像度」を縮小して教え、そこをクリックとか文字入力、と指定してもらってます。実際にはNut.js呼び出し時にリアル解像度に変換してからクリックしている。
- **処理速度やモデルレスポンス**  
  - AIモデルがワンステップごとにやりとりするので、操作がどうしても「1ステップ数秒」程度の遅さがあります。

---

## まとめ

**非エンジニア向け**には「AIが自分のPC上のアプリを自動操作する実験的ツール」だと理解できます。  
**エンジニア向け**には、Anthropic APIとNut.jsの組み合わせで「画面キャプチャをAIに送って操作ステップを返させる → Nut.jsでそれを実行 → その結果を再度AIに送る」というループが中心的なロジックである、という仕組みになっています。

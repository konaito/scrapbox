# [MultiOn](https://github.com/MULTI-ON/cookbook)

以下では、まず**非エンジニア向け**に「このCookbookを使って何ができるか」をざっくりとイメージできるように説明し、その後に**エンジニア向け**に「ロジックやカスタマイズの仕組み」をもう少し詳しく解説します。

---

## 非エンジニア向け：何ができるのか？

**MultiOn Cookbook** は、ブラウザ操作や情報取得を“AIエージェント”に任せることで、以下のようなタスクを**自動化**または**効率化**できるサンプル集です。

1. **ウェブサイト上での情報収集やスクレイピング**  
   - *例*: ニュースサイトから記事や要約を取得（`news-digest`）
   - *例*: LinkedInの検索結果からプロフィール情報を収集（`scraping`）
   - *例*: Hacker Newsを再帰的に巡回し投稿やコメントを収集（`recursive-scraping`）

2. **ブラウザでの操作を自動化して予約や申し込みをする**  
   - *例*: Airbnbで宿泊施設を探し予約する（`accommodation`）
   - *例*: OpenTableでレストラン予約を行う（`restaurant-booking`）

3. **Web上の作業を“エージェント”に委任して複数のタスクを並列実行する**  
   - *例*: SNS投稿を複数同時に行う、あるいは競合リサーチをまとめて行う（`internet-of-agents`）
   - *例*: 求人検索と応募をまとめて実行する（`job-search`）

4. **コーディング作業や調査、ノート取りなどをサブエージェントに割り当てる**  
   - *例*: “プログラマー”“リサーチャー”“ノートテイカー”の3種エージェントを連携させ、Webリサーチしながらコードを書く作業を補助（`coding-agent`）

5. **個人の嗜好や過去のデータを活用して提案**  
   - *例*: [Mem0](https://mem0.ai)等からユーザーの過去メモを取得して“パーソナライズされた旅行プラン”を提案（`personalized-travel-agent`）

**ポイントは「実際にブラウザを操作する」「特定の操作を繰り返し自動化する」「スクレイピングや入力などに連続的に対応する」こと**です。  
ユーザーは、シンプルな自然言語で「●●して欲しい」「検索結果から△△を集めて」「一番条件の良いプランを選んで予約して」といった指示を出すだけで、AIエージェントがステップバイステップで実行してくれます。

---

## エンジニア向け：ロジックの概要とカスタマイズポイント

### 1. **環境・依存関係**

- すべてのプロジェクトで**Poetry**が採用されており、`pyproject.toml` で依存パッケージ（`multion`, `openai`, `python-dotenv`など）が管理されています。  
- 多くの例で「`local=True`（ブラウザ拡張を使う形）」と「`remote`モード（サーバー実行）」の両方のパターンがあり、ブラウザ操作が必要なら`local=True`が指定されます。

### 2. **MultiOn Clientの基本メソッド**

**`client = MultiOn(api_key=...)`**  
- `api_key`を指定してMultiOnのクライアントを生成します。

**`client.sessions.create(url=..., local=...)`**  
- 指定したURLで新規ブラウザセッション（エージェント）を立ち上げます。  
- `local=True` なら自分のPCのブラウザを、`local=False` (または省略) ならリモート環境でブラウザ操作を行います。

**`client.sessions.step(session_id=..., cmd=...)`**  
- 既存セッションに対して、人間がブラウザ上でするような「1ステップ」を自然言語で指示（`cmd`）します。  
- セッション内のエージェントがフォーム入力、ボタン押下、ページ移動などを自動実行し、結果が返ってきます。

**`client.retrieve(..., cmd=..., fields=[...])`**  
- 多くの例で使われる「スクレイピング用」の操作。スクロールやJavaScriptレンダリングに対応しつつ、指定したページ要素をまとめて構造化データとして返します。  
- `scroll_to_bottom=True` でページ下部まで自動スクロールし、Lazy Loadされた要素を取得するサンプルが多いです。

### 3. **応用例：並列処理・再帰呼び出し**

- **並列実行**  
  例えば `ThreadPoolExecutor` や `prefect` を使い、一度に複数のページをスクレイピングする例がいくつか含まれています（`internet-of-agents` や `recursive-scraping`など）。  
  エージェント1つを各タスクに割り当てて並行して情報取得を行うことができます。

- **再帰的スクレイピング**  
  あるページのリンク先をさらに取得し、そこで新たな`retrieve`を呼ぶ、といった**再帰構造**のデモが `recursive_scraping.ipynb` に示されています。大規模サイトの深掘りができる点が利点です。

### 4. **複数エージェントの統括とプロンプト設計**

- `coding-agent` や `competitor-research` のようなフォルダでは、**「メインのオーケストレーター（管理エージェント）」**がユーザーからの指示を受け取り、必要に応じて
  - 「プログラマエージェントにコードを書かせる」
  - 「リサーチャーエージェントにWeb検索させる」
  - 「ノートテイカー（メモ取り）エージェントに情報を保管させる」  
  といった指示を投げ、最終的にタスクを完了させる流れが書かれています。

- **プロンプトエンジニアリング**  
  各エージェントに与えるプロンプト（`programmer_notes` や `researcher_notes` など）で、使用できるコマンドや注意点が細かく書かれています。これにより「余計な操作をしない」「ファイルの編集方法」「リロードしない」などの制限やガイドをエージェントに与えます。

### 5. **カスタマイズするときのポイント**

1. **APIキー切り替え**  
   - `.env` や `pyproject.toml` の設定によってAPIキーを渡す仕組みが多いです。自分の環境で`API_KEY`を設定してから動かす必要があります。

2. **対象サイトや検索条件**  
   - Notebookや`cmd`部分を自分の好みのキーワードやURLに変えれば、任意のサイトをスクレイピングできるようになります。

3. **必要なフィールドの指定**  
   - `retrieve` メソッドの `fields=[...]` で、「取得したい要素のラベル」や「構造的に欲しいデータ」を指定可能です。シンプルに「title」「url」「image_url」などのフィールド名に変えてみると良いでしょう。

4. **ステップ実行数やスクロール回数**  
   - 例では `scroll_to_bottom=True` としている場面が多いですが、大量データ取得時には件数制限やページごとの処理なども考慮が必要です。

5. **並列処理 or 単一処理**  
   - 一度に大量アクセスしたい場合は `ThreadPoolExecutor` を、1件ずつ安全に実行したい場合はシンプルにループで回す方法など、規模にあわせた実装を行えます。

---

## まとめ

- **非エンジニアの方**にとっては、「ブラウザ操作やスクレイピングをAIに任せる」ことで、競合調査、予約操作、求人応募などの単純作業やリサーチを効率化できる点がメリットです。  

- **エンジニアの方**は、Cookbookにある各サンプルコードをベースに「並列実行」「再帰スクレイピング」「複数エージェントのプロンプト設計」などの仕組みを理解し、自分用にカスタマイズしながら柔軟に機能を拡張できます。  

こうした設計のおかげで、簡単なユースケースから高度なシステム開発までスケールアップが可能です。ぜひサンプルを手がかりに、独自のウェブ操作・スクレイピングエージェントを作ってみてください。
